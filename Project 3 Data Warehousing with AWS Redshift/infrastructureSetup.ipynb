{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "from botocore.exceptions import ClientError\n",
    "import configparser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster Setup Procedures\n",
    "#### load credentials and parameters\n",
    "from the `dwh` config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read_file(open('dwh.cfg'))\n",
    "\n",
    "ACCESS_KEY_ID = config.get('aws_credentials','access_key_id')\n",
    "SECRET_ACCESS_KEY = config.get('aws_credentials','secret_access_key')\n",
    "\n",
    "REDSHIFT_ROLE_NAME = config.get('cluster_settings','redshift_role_name')\n",
    "CLUSTER_TYPE = config.get('cluster_settings','cluster_type')\n",
    "NUMBER_OF_NODES = config.get('cluster_settings','number_of_nodes')\n",
    "NODE_TYPE = config.get('cluster_settings','node_type')\n",
    "CLUSTER_IDENTIFIER = config.get('cluster_settings','cluster_identifier')\n",
    "DB_NAME = config.get('cluster_settings','db_name')\n",
    "MASTER_USER_NAME = config.get('cluster_settings','master_user_name')\n",
    "MASTER_USER_PASSWORD = config.get('cluster_settings','master_user_password')\n",
    "PORT = config.get('cluster_settings','port')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### create clients/resources\n",
    "for the IAM, EC2, S3 and Redshift services we'll use.  \n",
    "\n",
    "For demonstration purposes, the State of Oregon (\"us-west-2\") is set as the default region providing our AWS Services."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "iam = boto3.client(\n",
    "     'iam'\n",
    "    ,region_name = 'us-west-2'\n",
    "    ,aws_access_key_id = ACCESS_KEY_ID\n",
    "    ,aws_secret_access_key = SECRET_ACCESS_KEY\n",
    ")\n",
    "\n",
    "ec2 = boto3.resource(\n",
    "     'ec2'\n",
    "    ,region_name = 'us-west-2'\n",
    "    ,aws_access_key_id = ACCESS_KEY_ID\n",
    "    ,aws_secret_access_key = SECRET_ACCESS_KEY\n",
    ") \n",
    "\n",
    "s3 = boto3.resource(\n",
    "     's3'\n",
    "    ,region_name = 'us-west-2'\n",
    "    ,aws_access_key_id = ACCESS_KEY_ID\n",
    "    ,aws_secret_access_key = SECRET_ACCESS_KEY\n",
    ") \n",
    "\n",
    "redshift = boto3.client(\n",
    "     'redshift'\n",
    "    ,region_name = 'us-west-2'\n",
    "    ,aws_access_key_id = ACCESS_KEY_ID\n",
    "    ,aws_secret_access_key = SECRET_ACCESS_KEY\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create an IAM role\n",
    "granting Read-Only access for Redshift to an S3 bucket\n",
    "\n",
    "> [IAM Roles documentation](<https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles.html>)\n",
    "  \n",
    "> [IAM objects guidelines](<https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_iam-limits.html>)\n",
    "  \n",
    "> [boto3 IAM `create_role()` method](<https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/iam.html#IAM.Client.create_role>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'dwhRole' IAM role succesfully created.\n",
      "'AmazonS3ReadOnlyAccess' policy successfully attached to 'dwhRole'\n",
      "'dwhRole' ARN successfully retrieved.\n"
     ]
    }
   ],
   "source": [
    "# create the IAM Role\n",
    "try:    \n",
    "    iam.create_role(\n",
    "         Path='/'\n",
    "        ,RoleName=REDSHIFT_ROLE_NAME\n",
    "        ,Description='Allows Redshift access to S3 Buckets'\n",
    "        ,AssumeRolePolicyDocument=json.dumps({\n",
    "            'Statement':[{\n",
    "                 'Action':'sts:AssumeRole'\n",
    "                ,'Effect':'Allow'\n",
    "                ,'Principal':{\n",
    "                    'Service':'redshift.amazonaws.com'\n",
    "                }\n",
    "            }]\n",
    "        ,'Version': '2012-10-17'\n",
    "        })    \n",
    "    )\n",
    "    \n",
    "    print(\"'dwhRole' IAM role succesfully created.\")\n",
    "    \n",
    "except Exception as exception:\n",
    "    print(exception)\n",
    "    \n",
    "iam.attach_role_policy(\n",
    "     RoleName=REDSHIFT_ROLE_NAME\n",
    "    ,PolicyArn='arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess'    \n",
    ")\n",
    "print(\"'AmazonS3ReadOnlyAccess' policy successfully attached to 'dwhRole'\")\n",
    "\n",
    "REDSHIFT_ROLE_ARN = iam.get_role(RoleName=REDSHIFT_ROLE_NAME)['Role']['Arn']\n",
    "print(\"'dwhRole' ARN successfully retrieved.\")\n",
    "\n",
    "# role ARN is stored within ConfigParser for later storage in config file\n",
    "config.set('cluster_settings','redshift_role_arn',REDSHIFT_ROLE_ARN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### spin up a Redshift cluster instance:\n",
    "for demonstration purposes it is defined as publicly accessible.\n",
    "> [Redshift's boto3 `create_cluster()` documentation](<https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/redshift.html#Redshift.Client.create_cluster>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "try:\n",
    "    redshift.create_cluster(\n",
    "        # cluster settings\n",
    "         ClusterIdentifier = CLUSTER_IDENTIFIER\n",
    "        ,ClusterType = CLUSTER_TYPE\n",
    "        ,NodeType = NODE_TYPE\n",
    "        ,NumberOfNodes = int(NUMBER_OF_NODES)\n",
    "        ,IamRoles = [REDSHIFT_ROLE_ARN]\n",
    "        ,PubliclyAccessible = True\n",
    "        \n",
    "        # database settings\n",
    "        ,DBName = DB_NAME\n",
    "        ,MasterUsername = MASTER_USER_NAME\n",
    "        ,MasterUserPassword = MASTER_USER_PASSWORD        \n",
    "    )\n",
    "    \n",
    "except Exception as exception:\n",
    "    print(exception)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### use the `describe_clusters()` method\n",
    "to check cluster creation status and retrieve useful data on the cluster\n",
    "> [Redshift's boto3 `describe_clusters()` documentation](<https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/redshift.html#Redshift.Client.describe_clusters>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "available\n"
     ]
    }
   ],
   "source": [
    "cluster_status = redshift.describe_clusters(\n",
    "    ClusterIdentifier = CLUSTER_IDENTIFIER\n",
    ")\n",
    "\n",
    "print(cluster_status['Clusters'][0]['ClusterStatus'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_status = redshift.describe_clusters(\n",
    "    ClusterIdentifier = CLUSTER_IDENTIFIER\n",
    ")\n",
    "\n",
    "if cluster_status['Clusters'][0]['ClusterStatus'] == 'available':\n",
    "    \n",
    "    ENDPOINT = cluster_status['Clusters'][0]['Endpoint']['Address']\n",
    "\n",
    "    config.set('cluster_settings','endpoint',ENDPOINT)\n",
    "\n",
    "    # new data within ConfigParser is fed back to the credentials file\n",
    "    with open('dwh.cfg','w') as config_file:\n",
    "        config.write(config_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Open a TCP port\n",
    "to make the Cluster accessible via its Endpoint address:\n",
    "> [EC2 `Vpc`Class documentation](<https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/ec2.html#vpc>)  \n",
    "  \n",
    "> [AWS Docs on Virtual Private Cloud's (VPC) Security Groups](<https://docs.aws.amazon.com/vpc/latest/userguide/VPC_SecurityGroups.html>)\n",
    "  \n",
    "> [EC2 `Security Groups` boto3 examples](<https://boto3.amazonaws.com/v1/documentation/api/latest/guide/ec2-example-security-group.html>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    #  VPC Class instance is created. Its Id is fetched\n",
    "    # from the \"cluster_status\" object created above.\n",
    "    vpc = ec2.Vpc(id=cluster_status['Clusters'][0]['VpcId'])\n",
    "\n",
    "    #  The \"security_groups\" attribute is used in conjunction\n",
    "    # with its \"all()\" method to return a Collection containing\n",
    "    # the Security Groups available. It's then converted into\n",
    "    # a list object.\n",
    "    security_groups_list = list(vpc.security_groups.all())\n",
    "\n",
    "    for sec_group in security_groups_list:\n",
    "        \n",
    "        print('Authorizing security group:',sec_group.group_name)\n",
    "\n",
    "        try:\n",
    "            sec_group.authorize_ingress(\n",
    "                 GroupName=sec_group.group_name\n",
    "                ,CidrIp='0.0.0.0/0'\n",
    "                ,IpProtocol='TCP'\n",
    "                ,FromPort=int(PORT)\n",
    "                ,ToPort=int(PORT)\n",
    "            )\n",
    "\n",
    "        except Exception as exception:\n",
    "            print(exception)\n",
    "            \n",
    "except Exception as exception:\n",
    "    print(exception)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IPython-SQL is used\n",
    "to check whether a connection to the Cluster is possible  \n",
    "> [IPython-SQL github repo and documentation](<https://github.com/catherinedevlin/ipython-sql>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(psycopg2.OperationalError) could not connect to server: Connection timed out (0x0000274C/10060)\n",
      "\tIs the server running on host \"dwhcluster.cnoetczdeddb.us-west-2.redshift.amazonaws.com\" (52.10.228.129) and accepting\n",
      "\tTCP/IP connections on port 5439?\n",
      "\n",
      "(Background on this error at: http://sqlalche.me/e/e3q8)\n",
      "Connection info needed in SQLAlchemy format, example:\n",
      "               postgresql://username:password@hostname/dbname\n",
      "               or an existing connection: dict_keys([])\n"
     ]
    }
   ],
   "source": [
    "%load_ext sql\n",
    "\n",
    "conn_string = f'postgresql://{MASTER_USER_NAME}:{MASTER_USER_PASSWORD}@{ENDPOINT}:{PORT}/{DB_NAME}'\n",
    "        \n",
    "%sql $conn_string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster Take Down Procedures\n",
    "#### the Redshift Cluster instance\n",
    "is unprovisioned using its `delete_cluster()` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Cluster': {'ClusterIdentifier': 'dwhcluster',\n",
       "  'NodeType': 'dc2.large',\n",
       "  'ClusterStatus': 'deleting',\n",
       "  'ClusterAvailabilityStatus': 'Modifying',\n",
       "  'MasterUsername': 'sparkify',\n",
       "  'DBName': 'sparkify_dw',\n",
       "  'Endpoint': {'Address': 'dwhcluster.cnoetczdeddb.us-west-2.redshift.amazonaws.com',\n",
       "   'Port': 5439},\n",
       "  'ClusterCreateTime': datetime.datetime(2020, 6, 22, 23, 28, 0, 95000, tzinfo=tzutc()),\n",
       "  'AutomatedSnapshotRetentionPeriod': 1,\n",
       "  'ManualSnapshotRetentionPeriod': -1,\n",
       "  'ClusterSecurityGroups': [],\n",
       "  'VpcSecurityGroups': [{'VpcSecurityGroupId': 'sg-0c94b155',\n",
       "    'Status': 'active'}],\n",
       "  'ClusterParameterGroups': [{'ParameterGroupName': 'default.redshift-1.0',\n",
       "    'ParameterApplyStatus': 'in-sync'}],\n",
       "  'ClusterSubnetGroupName': 'default',\n",
       "  'VpcId': 'vpc-674bd61f',\n",
       "  'AvailabilityZone': 'us-west-2a',\n",
       "  'PreferredMaintenanceWindow': 'sun:07:00-sun:07:30',\n",
       "  'PendingModifiedValues': {},\n",
       "  'ClusterVersion': '1.0',\n",
       "  'AllowVersionUpgrade': True,\n",
       "  'NumberOfNodes': 2,\n",
       "  'PubliclyAccessible': True,\n",
       "  'Encrypted': False,\n",
       "  'Tags': [],\n",
       "  'EnhancedVpcRouting': False,\n",
       "  'IamRoles': [{'IamRoleArn': 'arn:aws:iam::750424551752:role/dwhRole',\n",
       "    'ApplyStatus': 'in-sync'}],\n",
       "  'MaintenanceTrackName': 'current',\n",
       "  'DeferredMaintenanceWindows': [],\n",
       "  'NextMaintenanceWindowStartTime': datetime.datetime(2020, 6, 28, 7, 0, tzinfo=tzutc())},\n",
       " 'ResponseMetadata': {'RequestId': '19296369-4fb0-450e-99f5-3770ecdf35ab',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '19296369-4fb0-450e-99f5-3770ecdf35ab',\n",
       "   'content-type': 'text/xml',\n",
       "   'content-length': '2391',\n",
       "   'vary': 'accept-encoding',\n",
       "   'date': 'Tue, 23 Jun 2020 00:35:25 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "redshift.delete_cluster(\n",
    "     ClusterIdentifier=CLUSTER_IDENTIFIER\n",
    "    ,SkipFinalClusterSnapshot=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### use Redshift's `describe_clusters()` method\n",
    "to monitor Cluster status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deleting\n"
     ]
    }
   ],
   "source": [
    "cluster_status = redshift.describe_clusters(\n",
    "    ClusterIdentifier = CLUSTER_IDENTIFIER\n",
    ")\n",
    "\n",
    "print(cluster_status['Clusters'][0]['ClusterStatus'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IAM Roles are also cleared\n",
    "by first detaching their previously attached policies and then effectively deleting them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': '2c1c069e-59c8-44a1-adca-358278210906',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '2c1c069e-59c8-44a1-adca-358278210906',\n",
       "   'content-type': 'text/xml',\n",
       "   'content-length': '200',\n",
       "   'date': 'Sat, 20 Jun 2020 23:18:07 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the S3 Read-Only policy is detached\n",
    "iam.detach_role_policy(\n",
    "     RoleName=REDSHIFT_ROLE_NAME\n",
    "    ,PolicyArn=\"arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess\"\n",
    ")\n",
    "\n",
    "# the Role is then deleted\n",
    "iam.delete_role(RoleName=REDSHIFT_ROLE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
